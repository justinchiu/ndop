\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Relation Extraction}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Problem Statement}
\begin{comment}
NOTE on RE vs KBP: KBP focuses on modeling the full database, whereas
in RE the emphasis is on explaining extractions.
KBP usually has a more complicated model over the KB,
while RE sometimes makes simplifying assumptions and could make a poor
KBP system.
Example: if triple double appears in text and someone scored < 10 PTS,
then it's more likely they more likely had > 10 REB.
The difference between RE and KBP is subtle. Both fall under
information extraction, and the models may overlap in many ways.
In fact, latent variable model approaches to RE are usually able to perform KBP as well.
However, the focus is usually not on having good aggregate extraction metrics,
but rather about identifying where facts are in text.
This leads to simplifying assumptions in the model of the KB itself.
On the other hand, KBP moves the difficulty from explaining extractions to
faithfully modeling the KB.
Typically this results in a challenging inference task, as the structure
of the KB may be very complex.
\end{comment}

Knowledge base population (KBP) is the task of converting information
conveyed in natural language into a structured representation,
such as a knowledge base (KB) consisting of keys and values.
By removing the noise inherent in natural language,
KBP systems produce information in a representation that is amenable to computation.

We extend the population task to both filling in the values of a KB
as well as providing a justification or `rationale' for the values.
We use these rationales to bias the model away from
relying on spurious correlations that may hurt generalization,
and provide human-checkable predictions.
%( garbage collection )

However, deciding what constitutes a rationale and
obtaining supervision for that specific definition is expensive.
For every definition, supervision must be 
obtained by having annotators consider many KB and text pairs.
This prevents one from iterating quickly in the space of rationale designs.
We therefore propose a definition of a rationale
and accompanying model that is able to produce rationales without supervision.

\begin{comment}
(Move or delete later)
A \textit{mention} is a surface realization of an abstract object in text.
In relation extraction we justify extractions by identifying fact mentions.
As text is noisy, the realization of a fact may be difficult to locate.
We focus on locating fact mentions at the word level by identifying
individual words as value mentions, rather than entity or type mentions.
We define a `word-level' decision, value, or process as pertaining to individual words,
while `sequence-level' refers to a single decision, value, or process for a whole sequence.
%(Why? Justify with generative model)
\end{comment}

\begin{comment}
Note on related work:
Except for \citet{zeng2018copy}, prior work has either assumed that the locations of
entities and values are given as input features or that the locations of entities and values
are observed at training time.
\end{comment}

The problem description is as follows:
Given a text $x = x_1, \ldots, x_{I}$ of length $I$
and the keys of an aligned KB $k = \set{k_j}_{j=1}^J$,
we model the values $v = \set{v_j}_{j=1}^J$.
Each $(k_j,v_j)$ pair is referred to as a record.
We must additionally produce a rationale for each record.

\begin{comment}
Modeling only the KB $(k,v)$ given the text $x$
is not sufficient, as our goal is to locate fact mentions.
We assume the KB contains many more facts than those mentioned in the text.
This fits many scenarios in real world applications:
We may have many entity and type key pairs in our KB,
but a summary may discuss only a small, salient subset of players and statistics.
We therefore propose a model that first identifies words as value mentions,
aligns those mentions to an entity and relation type in order to obtain a fact, 
and then aggregates word-level decisions into a sequence-level decision to resolve conflicts.
%We choose to identify value mentions as the location of a fact as entity mentions
%may be obscured through coreference and relation types may not be mentioned explicitly.
\end{comment}

Consider the following example:
Let our KB consist of information about a basketball game,
$$
k = \left\{\begin{array}{c}
    (\textrm{John Doe}, \textrm{POINTS}), \\
    (\textrm{John Doe}, \textrm{REBOUNDS}), \\
    (\textrm{John Doe}, \textrm{FIRST\_NAME}),\\
    (\textrm{John Doe}, \textrm{LAST\_NAME}), \\
    \vdots
\end{array}\right\},
v = \left\{\begin{array}{c}
8,\\
12,\\
\texttt{`John'},\\
\texttt{`Doe'},\\
\vdots 
\end{array}\right\}
$$
aligned to the summary $x = \texttt{`John Doe scored eight points'}$.
We identify the value mention
\texttt{`eight'} as the mention of the record $(k_1, v_1)$.
The record $(k_2,v_2)$ is not mentioned in the text.

\subsection{Model}
We choose our definition of a rationale:
A value for a specific key must be accompanied by the position
of its mention in the text.
Additionally, a value not supported by a value mention in the text
must also be made explicit.

To satisfy this definition of a rationale, we model the values in the KB
with an extract-then-aggregate process.
Our model first makes extractions for each word, then aggregates the word-level choices
in order to resolve possible conflicts.

For every word in the text, the word-level extraction model chooses whether that word is a value mention,
what key the value mention aligns to, and finally what value the mention corresponds to.
We introduce the following latent variables in the extraction model:
\begin{itemize}
\item $m = m_1, \ldots, m_I$ where each $m_i \in \set{0,1}$
    indicates whether word $x_i$ is part of a value mention.
    We aim to identify at least one word in a multiword mention.
\item $a = a_1, \ldots, a_I$ where each
    $a_i \in \set{1, \ldots, J} \cup \set{\texttt{None}}$
    indicates that word $x_i$ mentions the record $(k_{a_i}, v_{a_i})$.
\item $z = z_1, \ldots, z_I$ where each $z_i \in \mcV\cup \set{\texttt{None}}$
    translates the value mention containing word $x_i$
    into the canonical representation of
    the value as determined by the schema of the KB.
    For example, the KB may only store numbers in numerical form,
    as opposed to alphabetical.
\end{itemize}

Our extract-then-aggregate model specifies the joint distribution over the values $v$ and latent variables $z,a,m$
given the text $x$ and keys $k$:
\begin{equation}
\label{eqn:prob}
\begin{aligned}
&p(v,z,a,m\mid x,k)\\
%&= p(v\mid z,a,m,x,k)p(z\mid x)p(a\mid x,k)p(m\mid x)\\
&= p(v\mid z,a,m,x,k)p(z,a,m\mid x,k)\\
&= \left(\prod_{j=1}^J \underbrace{p(v_j\mid z,a,m,x,k)}_{\textrm{Aggregation}}\right)
    \underbrace{\left(\prod_{i=1}^I
        \underbrace{p(z_i\mid m_i,x)}_{\textrm{Translation}}
        \underbrace{p(a_i\mid m_i,x,k)}_{\textrm{Alignment}}
        \underbrace{p(m_i\mid x)}_{\textrm{Identification}}
    \right)}_{\textrm{Extraction}}
\end{aligned}
\end{equation}

The extraction model has three components.
The model must identify whether word $x_i$
is a mention using the \textit{identification} model $p(m_i \mid x)$,
align word $x_i$ to a key $k_{a_i}$ via the \textit{alignment} model $p(a_i \mid m_i, x, k)$,
and translate the word $x_i$ into a value $z_i$ with the
\textit{translation} model $p(z_i \mid m_i, x)$.

This model assumes that each word $x_i$ can explain at most one record in the KB.
Additionally, we make the simplifying assumption
that a value mention's translation is unambiguous and independent from the alignment to a key.

It is possible for there to be conflicts in the word-level extraction model if 
two words are mentions with the same alignment but different values.
The aggregation model $p(v_j \mid z,a,m,x,k)$ is necessary to resolve these conflicts,
and uses the per-word latent variables $z_i,a_i,m_i$ to predict the values of the KB.

We give the parameterizations of all conditional distributions in the following section.

\begin{figure}[h]
\begin{center}
\resizebox {.35\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Y) {$x$};
\node[latent, below=of Y](Ai) {$a_i$};
\node[latent, left=of Ai](Zi) {$z_i$};
\node[latent, right=of Ai](Mi) {$m_i$};

%\node[obs, below=of A] (D) {$k_j$};
%\node[obs, below=of Vi] (V) {$v_j$};
\node[obs, below=of Ai] (K) {$k$};
\node[obs, below=of Zi] (V) {$v$};

\plate {i} {(Zi)(Ai)(Mi)} {$I$};
%\plate  {j} {(D)(V)} {$J$};

\draw[->] (Y) -- (Mi);
\draw[->] (Y) -- (Ai);
\draw[->] (Y) -- (Zi);
%\draw[->] (A) -- (Vi);
\draw[->] (Mi) to [bend left = 25] (Zi);
\draw[->] (Mi) to [bend left = 25] (Ai);
\draw[->] (K) -- (Ai);
%\draw[-] (Vi) -- (V);
%\draw[-] (A) -- (V);
%\draw[-] (C) -- (V);
%\draw[-] (E) to [bend right = 25] (V);
%\draw[-] (T) -- (V);
\draw[->] (Zi) -- (V);
\draw[->] (Ai) -- (V);
\draw[->] (Mi) -- (V);
\draw[->] (K) -- (V);

\end{tikzpicture}
} %% end resize
\end{center}
\caption{Our model predicts word-level values and alignments
then aggregates those choices over all indices $i$ to
make a single decision for each value.
Each word has the following latent variables:
the mention $m_i \in \set{0,1}$ indicates whether word $x_i$ is a value mention,
the alignment $a_i$ gives the key $k_{a_i}$ that $x_i$ aligns to,
and the value $z_i$ gives the canonical value that $x_i$ translates to.
}
\label{fig:infmodel}
\end{figure}

\subsection{Parameterization}
Let $\bh_i \in \R^d$ be a contextual embedding of the word $x_i$,
and $E$ an embedding function that maps keys $k$ to vectors in $\R^{d}$.
\begin{enumerate}
\item Identification: We use the contextual embedding to  predict
whether a word $x_i$ is part of a value mention.
\begin{equation}
p(m_i \mid x) \propto \exp(W_m\bh_i)_{m_i},
\end{equation}
with $W_m \in \R^{2 \times d}$.
\item Alignment: We use a bilinear function of the cell embeddings and
contextual embeddings to parameterize the alignment distribution.
We align $a_i = \texttt{None}$ if a word is not a mention.
\begin{equation}
\begin{aligned}
p(a_i \mid m_i=1,x,k) &\propto \exp(E(k_{a_i})^T W_a \bh_i)\\
p(a_i=\texttt{None} \mid m_i=0,x,k) &= 1
\end{aligned}
\end{equation}
with $W_a \in \R^{d \times d}$.
\item Translation: We use the contextual embedding to translate a word
into a value. 
If a word is chosen not to be a mention such that $m_i=0$,
we set $z_i = \texttt{None}$.
\begin{equation}
\begin{aligned}
p(z_i \mid m_i=1, x) &\propto \exp(W_z \bh_i)_{z_i}\\
p(z_i = \texttt{None} \mid m_i=0, x) &= 1
\end{aligned}
\end{equation}
with $W_z \in \R^{|\mcV| \times d}$.
\item Aggregation:
We choose the value associated with a particular key in the KB
by first using the translated values of any mentions aligned to the
corresponding key.
If no mention is aligned to the key, we choose the value given only the key.

Formally, the conditional distribution is given by
\begin{align}
p(v_j \mid z,a,m,k) &\propto \begin{cases}
    \prod_{i}
        \exp(\psi(v_j, z_i,a_i,m_i)),  & \exists i, m_i = 1 \wedge a_i = j\\
    %p(v_j \mid e_j,t_j), & \textrm{otherwise}
    \exp(E(v_j)^TW_v [E(k_j)]), & \textrm{otherwise}
\end{cases}\\
\psi(v_j, z_i, a_i, m_i) &= \mathbf{1}(v_j = z_i, a_i = j, m_i=1)%\\
%p(v_j \mid e,t) & \propto \exp(E(v_j)^TW_v [E(e),E(t)])
\end{align}
with $W_v \in \R^{d\times d}$.
\end{enumerate}

\section{Training and Inference}
We would like to optimize the marginal likelihood of the values $v$ given the 
text $x$ and keys $k$:
\begin{equation}
\log p(v \mid x,k) = \log \sum_{z,a,m} p(v,z,a,m \mid x,k)
\end{equation}
However, maximizing $\log p(v \mid x,k)$ directly is very expensive 
as the summation over variables $z,a,m$ is intractable;
in particular,
the summation has computational complexity $O((|\mcV|\cdot J)^{I})$.
We therefore resort to approximate inference,
specifically amortized variational inference.

\subsection{Inference Network}
We introduce an inference network $q(z,a,m\mid v,x,k)$
and optimize the following lower bound on the marginal likelihood
with respect to the parameters of both $p$ and $q$:
\begin{equation}
\label{eqn:lowerbound}
\log p(v\mid x,k) \geq
\Es{q(z,a,m\mid v,x,k)}{\log \frac{p(v,z,a,m\mid x,k)}{q(z,a,m\mid v,x,k)}}
\end{equation}

The joint distribution of $q(z,a,m\mid v,x,k)$ in the inference network is given by
\begin{equation}
\label{eqn:elbo}
\begin{aligned}
q(z,a,m\mid v,x,k) &= \prod_i
    \underbrace{q(z_i \mid a,v,x)}_{\textrm{Translation}}
    \underbrace{q(a_i \mid v,x,k)}_{\textrm{Alignment}}
    \underbrace{q(m_i \mid v,x)}_{\textrm{Identification}}
\end{aligned}
\end{equation}

In the inference network,
we use the contextual embedding $\bh_i \in \R^d$ of the word $x_i$
and extend the embedding function $E$ to embed not only keys but also values in $\R^d$.
We introduce the following attention weights over records
in order to get a weighted representation of the KB for each word $x_i$:
\begin{equation}
\label{eqn:attn}
\begin{aligned}
\bg_j &= [E(k_j); E(v_j)]\\
\alpha_{ij} &\propto \exp(\bg_{j}^T W_\alpha \bh_i)
\end{aligned}
\end{equation}
with $W_\alpha \in \R^{2d \times d}$.

We propose to parameterize the inference network $q(z,a,m\mid v,x,k)$ as follows:
\begin{enumerate}
\item Identification: To predict whether $x_i$ is a mention,
    the inference network uses the weighted representation of the KB $\alpha$
    along with the contextual representation $\bh_i$
    \begin{equation}
    p(m_i \mid v,x) \propto
    \exp\left(V_m
    \textrm{MLP}\left(\left[\sum_j \alpha_{ij} \cdot \bg_j; \bh_i\right]\right)\right)_{m_i}
    \end{equation}
    where MLP is a neural network and $V_m \in \R^{2 \times d}$.
\item Alignment: The alignment model uses the attention weights in Eqn.~\ref{eqn:attn}:
    $q(a_i = j \mid v,x,k) = \alpha_{ij}$.
\item Translation: The translation model
    $q(z_i \mid a_i,v,x) = \mathbf{1}(z_i = v_{a_i})$
    conditions on the alignment $a_i$ and ensures the translated value $z_i$ is consistent
    with the alignment. 
\end{enumerate}

We optimize the objective in Eqn.~\ref{eqn:elbo}
with respect to both the extraction model $p$ and the inference network $q$
via gradient ascent using the score function gradient estimator.
We utilize a leave-one-out baseline for variance reduction.

(Under consideration, could hopefully remove pretraining by parameterizing translation
model using local features such as embedding or conv, but previous
experiments on this front were negative)
As our model is highly unidentifiable,
we bias our extraction model by pretraining $p(z_i \mid x)$ on lexical match.

\section{Evaluation}
We evaluate whether the model can discover
and locate the subset of records in a KB that are expressed in the text
without observing the values of the KB.
We compare the extractions of our model with a ground truth set of records,
reporting the micro-averaged precision, recall, and F1 score.
%We also consider as well as the location of their value mention in text.

We perform extraction by finding
$${\arg\max}_{z,a,m} p(z,a,m\mid x,k).$$
We obtain the record $(k_j, v_j)$ at word $x_i$ if $m_i=1$,
$a_i = j$ and $z_i = v_j$.
% Can also use approximate posterior
%The extraction is considered correct if the ground truth
%contains a fact with the same entity, type, value, and location.
%Alternatively, we could condition on an existing database
%As we assumed a fully factored model over $z,a,m$ 

\begin{comment}

\subsection{Approximate the posterior of a generative model}
Alternatively, we may introduce a generative model of text $q(x, v)$ 
that inverts the relation extraction model $p(v \mid x)$ and optimize the following objective:
\begin{equation}
\log q(x,v) - KL[p(v,z,a,m \mid x) || q(v,z,a,m \mid x)]
\end{equation}
which entails training the generative model of text while approximating its
posterior with the information extraction system.

decompose the training of our extraction system $p(v\mid x)$ into two stages:
In the first stage we train $p(z,a,m\mid x,d)$ to approximate the posterior
of a conditional model of text given a complete KB $q(x,z,a,m \mid v,d)$.
This has the benefit of allowing us to exert control over where value mentions are detected
through our design of the text model $q$.

In the second stage, we have two choices:
a) train $p(v \mid z,a,m,x,d)$ to approximate the posterior of a full generative model of text and
the values of KB $q(x,v\mid d)$.
b) train $p(v \mid z,a,m,x,d)$ using the following lower bound:
\begin{equation}
\label{eqn:lowerbound2}
\log p(v\mid x,k) \geq
\Es{p(z,a,m\mid x,d)}{\log p(v\mid z,a,m, x,d)}
\end{equation}
Ideally the bound in Eqn.~\ref{eqn:lowerbound2}
should not be looser than the one presented in Eqn.~\ref{eqn:lowerbound},
as conditioning on the observed values of a KB should not reduce the entropy of
a good alignment model.

How do we split the gradient over time?

\end{comment}

%\bibliography{bib}
%\bibliographystyle{acl_natbib}

\end{document}
